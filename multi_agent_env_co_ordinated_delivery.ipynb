{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open AI Gym Distribution Center Environment for Multi Agent Co-ordination to deliver packages using optimal routes\n",
    "\n",
    "#### Environment\n",
    "- This open AI gym environments split the city into 8 regions using directions, \n",
    "- Each region will have one agent to make local delivery or hand of packages to other remote regions \n",
    "- Each region will be assgined with packages either to deliver locally or deliver the products to other 7 regions Distribution centres.     \n",
    "- During every reset this environment randomly generate payload for every region for each agent.   0 is no load, 1 is load\n",
    "- Each agent can move left, stay in the same location or move right. \n",
    "- During every action package will be handed off to remote region or delivered locally depending on the actions.     \n",
    "- Rewards will be assigned based on the trip\n",
    "    - if agent makes successful hand-off or local delivery reward 1 will be assigned, \n",
    "    - if agent makes a trip but doesnâ€™t have package to hand off or locally delivery reward -1 will be assigned).  \n",
    "- Task will be considered done once all the packages are delivered. \n",
    "\n",
    "#### Regions and transition  (refer action_state_transition.csv) \n",
    "- 8 regions and corresponding idx North-0, North East-1, East-2, South East-3, South-4, South West-5, West-6, North West-7\n",
    "- When agent takes action 0 he moves to the left (anti clock direction) e.g if agent is in location North-0 and action is 0 agent moves to North West-7\n",
    "- When agent takes action 1 he stays in the same location e.g if agent is in location North-0 and action is 1 agents new location remains same 0\n",
    "- When agent takes action 2 he moves to the right (clock direction) e.g if agent is in location North-0 and action is 2 agent moves to North East-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "GlrFJuIJvI1_"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "9FiPlE0GvPE1"
   },
   "outputs": [],
   "source": [
    "from gym import spaces\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "class MultiAgentEnv(gym.Env):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.episode = 0\n",
    "        \n",
    "        ### State transition \n",
    "        self.state_transition = pd.read_csv('action_state_transition.csv')\n",
    "        self.agents_n = 8\n",
    "        self.start_time =  time.time()\n",
    "        self.log_time = time.time()\n",
    "        self.duration = 0\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        self.observation_space = spaces.Discrete(3*2*2*2*2*2*2*2*2)\n",
    "        self.state = self.reset()\n",
    "        self.reset()\n",
    "        \n",
    "    # Action  No Action - 0, Right action - 1, Left action -2\n",
    "    def step(self, action):\n",
    "        print_interval = 1*60\n",
    "\n",
    "        print_log = (time.time() - self.log_time) >= print_interval\n",
    "        if(print_log):\n",
    "            self.log_time = time.time()\n",
    "        self.episode = self.episode+1\n",
    "        # update inventory \n",
    "        new_loc = []\n",
    "        inventory = []\n",
    "        for i in range(len(action)):\n",
    "            new_loc.append(self.getNewLocation(self.state[i][0],action[i]))\n",
    "            inventory.append(list(self.state[i])[1:].copy())\n",
    "\n",
    "        #i = 0\n",
    "        rewards = []\n",
    "        for i in range(len(inventory)):\n",
    "            #inventory[i][new_loc[i]-1] = 0 # current agents inventory unload on remote/home location. \n",
    "            # if it is a home location it is final delivery so no need to increment the load\n",
    "            # if it is a remote location load needs to be transfered to corresponding agent.  agent id is equal to the location id. \n",
    "            # we have used only two agents 1 = north , 2 north east\n",
    "            reward = 0\n",
    "            if(new_loc[i]-1 in [i for i in range(8)]):\n",
    "                if (new_loc[i]-1 != i and inventory[i][new_loc[i]-1] > 0): # remote delivery\n",
    "                    #print('inventory before remote inventory update: {}'.format(inventory))\n",
    "                    inventory[i][new_loc[i]-1] = 0\n",
    "                    #print('remote delivery for agent: {} by agent {} : inventory: {}'.format(new_loc[i]-1,i,inventory))      \n",
    "                    \n",
    "                    #remote delivery for agent: 6 by agent 0            \n",
    "                    inventory[new_loc[i]-1][new_loc[i]-1] = 1\n",
    "                    #print('inventory after remote inventory update: {}'.format(inventory))\n",
    "                    reward = 1\n",
    "                elif inventory[i][new_loc[i]-1] > 0:\n",
    "                    #print('local delivery by agent: {}'.format(i))\n",
    "                    inventory[i][new_loc[i]-1] = 0\n",
    "                    reward = 1\n",
    "                else:\n",
    "                    #print('No Delivery')\n",
    "                    reward = -1\n",
    "            elif inventory[i][new_loc[i]-1] > 0:\n",
    "                #print('local delivery by agent: {}'.format(i))\n",
    "                inventory[i][new_loc[i]-1] = 0\n",
    "                reward = 1\n",
    "            else:\n",
    "                #print('No Delivery')\n",
    "                reward = -1\n",
    "            if(new_loc[i]-1 == i and sum(inventory[i])==0):\n",
    "                reward = 0\n",
    "            rewards.append(reward)\n",
    "\n",
    "        new_inventory = inventory\n",
    "        #print('new inventory after update: {}'.format(new_inventory))\n",
    "        #print(rewards)\n",
    "        print_log = False\n",
    "        if(print_log):\n",
    "            for k in range(len(rewards)):\n",
    "                print(\"Tries : {}, Current Location :{} , Current Inventory : {} ,action: {}, New Location : {}, Inventory drop for new Location: {}, Reward: {}\"\n",
    "                      .format(self.episode,self.state[k][0],self.state[k][1:],action[k],new_loc[k],self.state[k][1:][new_loc[k]-1],rewards[k]))\n",
    "        \n",
    "        new_state = list(self.state)\n",
    "        idx_of_new_loc_inventories = []\n",
    "        # update state with new position and inventory\n",
    "        for k in range(len(action)):\n",
    "            new_state_list = list(new_state[k])\n",
    "            new_state_list[0] = new_loc[k]\n",
    "            new_state_list[1:] = new_inventory[k]\n",
    "            new_state_tuple = tuple(new_state_list)\n",
    "            new_state[k] = new_state_tuple\n",
    "        \n",
    "        self.state  = [s for s in new_state]\n",
    "        done = False\n",
    "        remaning_inv = 0\n",
    "        for k in range(len(action)):\n",
    "            remaning_inv = remaning_inv + sum(self.state[k][1:])\n",
    "            \n",
    "        if(remaning_inv == 0):\n",
    "            done = True\n",
    "  \n",
    "        return self.state, rewards, done, {}\n",
    "    # North = 0, North East = 1, East = 2, South East = 3, South = 4, South West = 5, West = 6 , North West = 7\n",
    "    def getNewLocation(self,currentLocation,action):\n",
    "        return self.state_transition[self.state_transition['state']==currentLocation][self.state_transition['action']==action]['newstate'].iat[0]\n",
    "       \n",
    "    \n",
    "    # reward -1 if there is no delivery\n",
    "    # reward 1 if there is a delivery \n",
    "    # reward 0 for every step.  \n",
    "    \n",
    "    def get_rewards(self,newLocations,new_inventories,states):\n",
    "        rewards = []\n",
    "        for i in range(len(states)):\n",
    "            reward = 0\n",
    "            new_inventory=new_inventories[i]\n",
    "            old_inventory=states[i][1:]\n",
    "            old_location = states[i][0]\n",
    "            #print('agent : {} old and new inventory : {}, {}'.format(i,new_inventory,old_inventory))\n",
    "            #print('agent : {} sum of old and new inventory : {}, {}'.format(i,sum(new_inventory),sum(old_inventory)))\n",
    "            if(sum(new_inventory)<sum(old_inventory)):\n",
    "                reward = 1\n",
    "            else:\n",
    "                reward = -1\n",
    "            if(newLocations[i]==old_location and sum(old_inventory) == 0):\n",
    "                reward = 0\n",
    "            rewards.append(reward)\n",
    "        return rewards\n",
    "    def get_random_action(self):\n",
    "        return [np.random.randint(0,2) for i in range(self.agents_n)]\n",
    "\n",
    "    def reset(self):\n",
    "    \n",
    "        self.state = self.get_random_agent_states()\n",
    "        return self.state\n",
    "    #returns random states for each agent\n",
    "    def get_random_agent_states(self):\n",
    "        states = []\n",
    "        for i in range(self.agents_n):\n",
    "            state = (i,\n",
    "                     np.random.randint(0,2),\n",
    "                     np.random.randint(0,2),\n",
    "                     np.random.randint(0,2),\n",
    "                     np.random.randint(0,2),\n",
    "                     np.random.randint(0,2),\n",
    "                     np.random.randint(0,2),\n",
    "                     np.random.randint(0,2),\n",
    "                     np.random.randint(0,2))\n",
    "            states.append(state)\n",
    "        return states\n",
    "        \n",
    "    \n",
    "    def render(self, mode=\"human\"):\n",
    "        print(\"state :{} \".format(self.state))\n",
    "        \n",
    "    def close(self):\n",
    "        print(\"close\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "===========================================\n",
      "Old State: [(0, 0, 0, 0, 1, 0, 0, 0, 1), (1, 0, 1, 1, 1, 0, 0, 0, 1), (2, 1, 0, 0, 1, 1, 0, 0, 1), (3, 1, 0, 1, 0, 0, 0, 1, 0), (4, 0, 0, 0, 0, 0, 1, 0, 0), (5, 1, 0, 0, 0, 0, 0, 0, 0), (6, 1, 0, 0, 1, 0, 0, 1, 0), (7, 0, 1, 1, 1, 1, 0, 1, 1)]\n",
      "Actions : [0, 2, 0, 0, 2, 0, 2, 0]\n",
      "Average rewards : -0.25 Individual Rewards: [-1, 1, 1, -1, -1, -1, 1, -1]\n",
      "New State [(7, 1, 0, 0, 1, 0, 0, 0, 1), (2, 0, 0, 1, 1, 0, 0, 0, 1), (1, 0, 0, 0, 1, 1, 0, 0, 1), (2, 1, 0, 1, 0, 0, 0, 1, 0), (5, 0, 0, 0, 0, 0, 1, 0, 0), (4, 1, 0, 0, 0, 0, 0, 0, 0), (7, 1, 0, 0, 1, 0, 0, 0, 0), (6, 0, 1, 1, 1, 1, 0, 1, 1)] \n",
      "Task Complete: False \n",
      "\n",
      "Step 1\n",
      "===========================================\n",
      "Old State: [(7, 1, 0, 0, 1, 0, 0, 0, 1), (2, 0, 0, 1, 1, 0, 0, 0, 1), (1, 0, 0, 0, 1, 1, 0, 0, 1), (2, 1, 0, 1, 0, 0, 0, 1, 0), (5, 0, 0, 0, 0, 0, 1, 0, 0), (4, 1, 0, 0, 0, 0, 0, 0, 0), (7, 1, 0, 0, 1, 0, 0, 0, 0), (6, 0, 1, 1, 1, 1, 0, 1, 1)]\n",
      "Actions : [2, 2, 0, 2, 2, 0, 2, 0]\n",
      "Average rewards : 0.5 Individual Rewards: [1, 1, 1, 1, 1, -1, -1, 1]\n",
      "New State [(0, 1, 0, 0, 1, 0, 0, 0, 0), (3, 0, 0, 0, 1, 0, 0, 0, 1), (0, 0, 0, 1, 1, 1, 0, 0, 0), (3, 1, 0, 0, 0, 0, 0, 1, 0), (6, 0, 0, 0, 0, 1, 0, 0, 0), (3, 1, 0, 0, 0, 0, 1, 0, 0), (0, 1, 0, 0, 1, 0, 0, 0, 0), (5, 0, 1, 1, 1, 0, 0, 1, 1)] \n",
      "Task Complete: False \n",
      "\n",
      "Step 2\n",
      "===========================================\n",
      "Old State: [(0, 1, 0, 0, 1, 0, 0, 0, 0), (3, 0, 0, 0, 1, 0, 0, 0, 1), (0, 0, 0, 1, 1, 1, 0, 0, 0), (3, 1, 0, 0, 0, 0, 0, 1, 0), (6, 0, 0, 0, 0, 1, 0, 0, 0), (3, 1, 0, 0, 0, 0, 1, 0, 0), (0, 1, 0, 0, 1, 0, 0, 0, 0), (5, 0, 1, 1, 1, 0, 0, 1, 1)]\n",
      "Actions : [2, 2, 2, 0, 0, 0, 2, 0]\n",
      "Average rewards : 0.125 Individual Rewards: [1, 1, -1, -1, 0, -1, 1, 1]\n",
      "New State [(1, 1, 0, 0, 1, 0, 0, 0, 0), (4, 0, 0, 0, 0, 0, 0, 0, 1), (1, 0, 0, 1, 1, 1, 0, 0, 0), (2, 1, 0, 0, 1, 0, 0, 1, 0), (5, 0, 0, 0, 0, 0, 0, 0, 0), (2, 1, 0, 0, 0, 0, 1, 0, 0), (1, 0, 0, 0, 1, 0, 0, 0, 0), (4, 0, 1, 1, 0, 0, 0, 1, 1)] \n",
      "Task Complete: False \n",
      "\n",
      "Step 3\n",
      "===========================================\n",
      "Old State: [(1, 1, 0, 0, 1, 0, 0, 0, 0), (4, 0, 0, 0, 0, 0, 0, 0, 1), (1, 0, 0, 1, 1, 1, 0, 0, 0), (2, 1, 0, 0, 1, 0, 0, 1, 0), (5, 0, 0, 0, 0, 0, 0, 0, 0), (2, 1, 0, 0, 0, 0, 1, 0, 0), (1, 0, 0, 0, 1, 0, 0, 0, 0), (4, 0, 1, 1, 0, 0, 0, 1, 1)]\n",
      "Actions : [1, 0, 2, 0, 2, 0, 2, 0]\n",
      "Average rewards : 0.0 Individual Rewards: [1, -1, -1, 1, -1, 1, -1, 1]\n",
      "New State [(1, 1, 0, 0, 1, 0, 0, 0, 0), (3, 0, 0, 0, 0, 0, 0, 0, 1), (2, 0, 0, 1, 1, 1, 0, 0, 0), (1, 0, 0, 0, 1, 0, 0, 1, 0), (6, 0, 0, 0, 0, 0, 0, 0, 0), (1, 0, 0, 0, 0, 0, 1, 0, 0), (2, 0, 0, 0, 1, 0, 0, 0, 0), (3, 0, 1, 0, 0, 0, 0, 1, 1)] \n",
      "Task Complete: False \n",
      "\n",
      "Step 4\n",
      "===========================================\n",
      "Old State: [(1, 1, 0, 0, 1, 0, 0, 0, 0), (3, 0, 0, 0, 0, 0, 0, 0, 1), (2, 0, 0, 1, 1, 1, 0, 0, 0), (1, 0, 0, 0, 1, 0, 0, 1, 0), (6, 0, 0, 0, 0, 0, 0, 0, 0), (1, 0, 0, 0, 0, 0, 1, 0, 0), (2, 0, 0, 0, 1, 0, 0, 0, 0), (3, 0, 1, 0, 0, 0, 0, 1, 1)]\n",
      "Actions : [1, 0, 2, 0, 2, 0, 2, 0]\n",
      "Average rewards : -0.25 Individual Rewards: [1, -1, 1, -1, -1, -1, -1, 1]\n",
      "New State [(1, 0, 0, 0, 1, 0, 0, 0, 0), (2, 0, 1, 0, 0, 0, 0, 0, 1), (3, 0, 0, 0, 1, 1, 0, 0, 0), (0, 0, 0, 0, 1, 0, 0, 1, 0), (7, 0, 0, 0, 0, 0, 0, 0, 0), (0, 0, 0, 0, 0, 0, 1, 0, 0), (3, 0, 0, 0, 1, 0, 0, 0, 0), (2, 0, 0, 0, 0, 0, 0, 1, 1)] \n",
      "Task Complete: False \n",
      "\n",
      "Step 5\n",
      "===========================================\n",
      "Old State: [(1, 0, 0, 0, 1, 0, 0, 0, 0), (2, 0, 1, 0, 0, 0, 0, 0, 1), (3, 0, 0, 0, 1, 1, 0, 0, 0), (0, 0, 0, 0, 1, 0, 0, 1, 0), (7, 0, 0, 0, 0, 0, 0, 0, 0), (0, 0, 0, 0, 0, 0, 1, 0, 0), (3, 0, 0, 0, 1, 0, 0, 0, 0), (2, 0, 0, 0, 0, 0, 0, 1, 1)]\n",
      "Actions : [2, 0, 2, 0, 0, 0, 2, 0]\n",
      "Average rewards : -0.25 Individual Rewards: [-1, -1, 1, 1, -1, -1, 1, -1]\n",
      "New State [(2, 0, 0, 0, 1, 0, 0, 0, 0), (1, 0, 1, 0, 0, 0, 0, 0, 1), (4, 0, 0, 0, 0, 1, 0, 0, 0), (7, 0, 0, 0, 1, 0, 0, 0, 0), (6, 0, 0, 0, 0, 0, 0, 0, 0), (7, 0, 0, 0, 0, 0, 1, 0, 0), (4, 0, 0, 0, 0, 0, 0, 1, 0), (1, 0, 0, 0, 0, 0, 0, 1, 1)] \n",
      "Task Complete: False \n",
      "\n",
      "Step 6\n",
      "===========================================\n",
      "Old State: [(2, 0, 0, 0, 1, 0, 0, 0, 0), (1, 0, 1, 0, 0, 0, 0, 0, 1), (4, 0, 0, 0, 0, 1, 0, 0, 0), (7, 0, 0, 0, 1, 0, 0, 0, 0), (6, 0, 0, 0, 0, 0, 0, 0, 0), (7, 0, 0, 0, 0, 0, 1, 0, 0), (4, 0, 0, 0, 0, 0, 0, 1, 0), (1, 0, 0, 0, 0, 0, 0, 1, 1)]\n",
      "Actions : [2, 2, 2, 0, 2, 0, 2, 0]\n",
      "Average rewards : -0.125 Individual Rewards: [-1, 1, 1, -1, -1, 0, -1, 1]\n",
      "New State [(3, 0, 0, 0, 1, 0, 0, 0, 0), (2, 0, 0, 0, 0, 0, 0, 0, 1), (5, 0, 0, 0, 0, 0, 0, 0, 0), (6, 0, 0, 0, 1, 0, 0, 0, 0), (7, 0, 0, 0, 0, 1, 0, 0, 0), (6, 0, 0, 0, 0, 0, 0, 0, 0), (5, 0, 0, 0, 0, 0, 0, 1, 0), (0, 0, 0, 0, 0, 0, 0, 1, 0)] \n",
      "Task Complete: False \n",
      "\n",
      "Step 7\n",
      "===========================================\n",
      "Old State: [(3, 0, 0, 0, 1, 0, 0, 0, 0), (2, 0, 0, 0, 0, 0, 0, 0, 1), (5, 0, 0, 0, 0, 0, 0, 0, 0), (6, 0, 0, 0, 1, 0, 0, 0, 0), (7, 0, 0, 0, 0, 1, 0, 0, 0), (6, 0, 0, 0, 0, 0, 0, 0, 0), (5, 0, 0, 0, 0, 0, 0, 1, 0), (0, 0, 0, 0, 0, 0, 0, 1, 0)]\n",
      "Actions : [2, 0, 2, 0, 0, 2, 2, 0]\n",
      "Average rewards : -0.5 Individual Rewards: [1, -1, -1, -1, -1, -1, -1, 1]\n",
      "New State [(4, 0, 0, 0, 0, 0, 0, 0, 0), (1, 0, 0, 0, 0, 0, 0, 0, 1), (6, 0, 0, 0, 0, 0, 0, 0, 0), (5, 0, 0, 0, 1, 0, 0, 0, 0), (6, 0, 0, 0, 0, 1, 0, 0, 0), (7, 0, 0, 0, 0, 0, 0, 0, 0), (6, 0, 0, 0, 0, 0, 0, 1, 0), (7, 0, 0, 0, 0, 0, 0, 0, 0)] \n",
      "Task Complete: False \n",
      "\n",
      "Step 8\n",
      "===========================================\n",
      "Old State: [(4, 0, 0, 0, 0, 0, 0, 0, 0), (1, 0, 0, 0, 0, 0, 0, 0, 1), (6, 0, 0, 0, 0, 0, 0, 0, 0), (5, 0, 0, 0, 1, 0, 0, 0, 0), (6, 0, 0, 0, 0, 1, 0, 0, 0), (7, 0, 0, 0, 0, 0, 0, 0, 0), (6, 0, 0, 0, 0, 0, 0, 1, 0), (7, 0, 0, 0, 0, 0, 0, 0, 0)]\n",
      "Actions : [0, 0, 2, 0, 0, 0, 2, 0]\n",
      "Average rewards : -0.25 Individual Rewards: [-1, 1, -1, 0, 0, 0, 0, -1]\n",
      "New State [(3, 0, 0, 0, 0, 0, 0, 0, 0), (0, 0, 0, 0, 0, 0, 0, 0, 0), (7, 0, 0, 0, 0, 0, 0, 0, 0), (4, 0, 0, 0, 0, 0, 0, 0, 0), (5, 0, 0, 0, 0, 0, 0, 0, 0), (6, 0, 0, 0, 0, 0, 0, 0, 0), (7, 0, 0, 0, 0, 0, 0, 0, 0), (6, 0, 0, 0, 0, 0, 0, 0, 0)] \n",
      "Task Complete: True \n",
      "\n",
      "Toal Rewards : -1.0\n"
     ]
    }
   ],
   "source": [
    "##Test Scenario: Following scenario provides sample state and sequence of steps required to delivery all the packages.\n",
    "# State contains 8 rows to represent 8 agent stats. \n",
    "# Each agent states has 9 columns/values \n",
    "# \n",
    "#[currentlocaiton, \n",
    "#Load for region0 load for region1, \n",
    "#load for region2, load for region3, \n",
    "#load for region4, load for region5, \n",
    "#load for region6, load for region7]\n",
    "\n",
    "\n",
    "state = [(0, 0, 0, 0, 1, 0, 0, 0, 1), \n",
    "         (1, 0, 1, 1, 1, 0, 0, 0, 1),\n",
    "         (2, 1, 0, 0, 1, 1, 0, 0, 1),\n",
    "         (3, 1, 0, 1, 0, 0, 0, 1, 0),\n",
    "         (4, 0, 0, 0, 0, 0, 1, 0, 0), \n",
    "         (5, 1, 0, 0, 0, 0, 0, 0, 0), \n",
    "         (6, 1, 0, 0, 1, 0, 0, 1, 0), \n",
    "         (7, 0, 1, 1, 1, 1, 0, 1, 1)]\n",
    "\n",
    "# Contains multiple steps to complete the task\n",
    "# each step contains one action for each agent total 8 \n",
    "# Action 0 represent move to left, 1 represent no action, 2 represents move to right. \n",
    "steps = [[0, 2, 0, 0, 2, 0, 2, 0], [2, 2, 0, 2, 2, 0, 2, 0],\n",
    "         [2, 2, 2, 0, 0, 0, 2, 0], [1, 0, 2, 0, 2, 0, 2, 0], \n",
    "         [1, 0, 2, 0, 2, 0, 2, 0], [2, 0, 2, 0, 0, 0, 2, 0], \n",
    "         [2, 2, 2, 0, 2, 0, 2, 0], [2, 0, 2, 0, 0, 2, 2, 0],\n",
    "         [0, 0, 2, 0, 0, 0, 2, 0]]\n",
    "\n",
    "expected_reward = -1\n",
    "\n",
    "env = MultiAgentEnv()\n",
    "env.state = state\n",
    "total_rewards = 0\n",
    "\n",
    "for idx,step in enumerate(steps):\n",
    "    print('Step {}'.format(idx))\n",
    "    print('===========================================')\n",
    "    print('Old State: {}'.format(env.state))\n",
    "    print('Actions : {}'.format(step))\n",
    "    state, rewards, done, _ = env.step(step)\n",
    "    print('Average rewards : {} Individual Rewards: {}'.format(sum(rewards)/len(rewards),rewards))\n",
    "    print('New State {} '.format(state))\n",
    "    print('Task Complete: {} \\n'.format(done))\n",
    "    total_rewards = total_rewards + sum(rewards)/len(rewards)\n",
    "print('Toal Rewards : {}'.format(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "q_learning_notebook_local_store-2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
